{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLLEC 1 모음 210703.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9bbcVsSSfFsMklUBUG43W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JAEILJEON/JAEIL_ML_LAB/blob/main/MLLEC_1_%EB%AA%A8%EC%9D%8C_210704.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpn3JZn5RQJP"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQjmBaI0Weot"
      },
      "source": [
        "##ML LAB 4\n",
        "multi variable linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaeG4Y8RQYrv"
      },
      "source": [
        "data = np.array([\n",
        "    # X1,   X2,    X3,   y\n",
        "    [ 73.,  80.,  75., 152. ],\n",
        "    [ 93.,  88.,  93., 185. ],\n",
        "    [ 89.,  91.,  90., 180. ],\n",
        "    [ 96.,  98., 100., 196. ],\n",
        "    [ 73.,  66.,  70., 142. ]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# slice data\n",
        "X = data[:, :-1]\n",
        "y = data[:, [-1]]\n",
        "\n",
        "W = tf.Variable(tf.random_normal([3, 1]))\n",
        "b = tf.Variable(tf.random_normal([1]))\n",
        "\n",
        "learning_rate = 0.000001\n",
        "\n",
        "# hypothesis, prediction function\n",
        "def predict(X):\n",
        "    return tf.matmul(X, W) + b\n",
        "\n",
        "print(\"epoch | cost\")\n",
        "\n",
        "n_epochs = 2000\n",
        "for i in range(n_epochs+1):\n",
        "    # tf.GradientTape() to record the gradient of the cost function\n",
        "    with tf.GradientTape() as tape:\n",
        "        cost = tf.reduce_mean((tf.square(predict(X) - y)))\n",
        "\n",
        "    # calculates the gradients of the loss\n",
        "    W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "    # updates parameters (W and b)\n",
        "    W.assign_sub(learning_rate * W_grad)\n",
        "    b.assign_sub(learning_rate * b_grad)\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbkPP04nQijT",
        "outputId": "0a23305b-b604-426c-f8f8-960ff00b6511"
      },
      "source": [
        "#내가 해본 것\n",
        "data = np.array([\n",
        "    # X1,   X2,    X3,   y\n",
        "    [ 73.,  80.,  75., 152. ],\n",
        "    [ 93.,  88.,  93., 185. ],\n",
        "    [ 89.,  91.,  90., 180. ],\n",
        "    [ 96.,  98., 100., 196. ],\n",
        "    [ 73.,  66.,  70., 142. ]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# slice data\n",
        "X_data = data[:,:-1]\n",
        "Y_data = data[:,[-1]]   #여기 차원 처리해주는 게 좀 중요하네. 열 하나만 쓸 거면 딱 [-1] 이런 식으로 차원 지정해줘야겠다. 이거 안하면 shape이이상하게 돼서 broadcasting 됨.\n",
        "\n",
        "W = tf.Variable(tf.random.normal([3,1])) # 안에 값은 랜덤 지정인데, 3행 1열 지정해준 건 x(입력)이 3열, y(출력)가 1열이라서, W는 3행 1열.\n",
        "b = tf.Variable(tf.random.normal([1])) #b는 생략한 체로 많이 쓰긴 하지만 일단 있으니까 따라한다. #1버전에서 random_normal, 2버전에서는 random.normal()\n",
        "\n",
        "learning_rate = 0.0000001 #이건 원래 이렇게 작은 값으로 지정해줌.\n",
        "\n",
        "# hypothesis, prediction function\n",
        "def predict(X): #가설함수 세우는 거. H(X) = XW. matmul이나, dot 쓴다\n",
        "  return tf.matmul(X_data,W) + b\n",
        "\n",
        "n_epochs = 2000    #이건 반복횟수 정해주는 거~\n",
        "for i in range(n_epochs+1):    \n",
        "  # tf.GradientTape() to record the gradient of the cost function. 여기서부터 경사하강법 사용함. \n",
        "  with tf.GradientTape() as tape:\n",
        "    cost = tf.reduce_mean((tf.square(predict(X_data) - Y_data))) # 한번 더 묶어서 reduce_mean() 안에 들어가는 거.\n",
        "\n",
        "  # calculates the gradients of the loss\n",
        "  W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "  # updates parameters (W and b)\n",
        "  W.assign_sub(learning_rate * W_grad)\n",
        "  b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "  if i % 100 == 0: #100단위로 끊어서 출력해달라는 거임.\n",
        "    print(\"{:5} | {:10.4f}\".format(i, cost.numpy()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0 | 11961.0361\n",
            "  100 |  4951.3130\n",
            "  200 |  2050.2385\n",
            "  300 |   849.5856\n",
            "  400 |   352.6771\n",
            "  500 |   147.0248\n",
            "  600 |    61.9108\n",
            "  700 |    26.6852\n",
            "  800 |    12.1068\n",
            "  900 |     6.0730\n",
            " 1000 |     3.5757\n",
            " 1100 |     2.5423\n",
            " 1200 |     2.1146\n",
            " 1300 |     1.9375\n",
            " 1400 |     1.8641\n",
            " 1500 |     1.8337\n",
            " 1600 |     1.8211\n",
            " 1700 |     1.8158\n",
            " 1800 |     1.8136\n",
            " 1900 |     1.8126\n",
            " 2000 |     1.8122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4P6-LA2Q99T"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZchYJ1uWuaW"
      },
      "source": [
        "##ML LAB 05\n",
        "logistic regression / classfication 텐서플로우로 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "_3fe4oZ0Ur0O",
        "outputId": "c2e6a8c3-cfd2-43f8-a4e9-6c0f75b4e20b"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(777)  # for reproducibility\n",
        "print(tf.__version__)\n",
        "\n",
        "\n",
        "\n",
        "x_train = [[1., 2.],\n",
        "          [2., 3.],\n",
        "          [3., 1.],\n",
        "          [4., 3.],\n",
        "          [5., 3.],\n",
        "          [6., 2.]]\n",
        "y_train = [[0.],\n",
        "          [0.],\n",
        "          [0.],\n",
        "          [1.],\n",
        "          [1.],\n",
        "          [1.]]\n",
        "\n",
        "x_test = [[5.,2.]]\n",
        "y_test = [[1.]]\n",
        "\n",
        "\n",
        "x1 = [x[0] for x in x_train] #xtrain의 0번 위치 값들\n",
        "x2 = [x[1] for x in x_train]  #xtrain의 1번 위치 값들\n",
        "\n",
        "colors = [int(y[0] % 3) for y in y_train]\n",
        "plt.scatter(x1,x2, c=colors , marker='^')\n",
        "plt.scatter(x_test[0][0],x_test[0][1], c=\"red\")\n",
        "\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.show()\n",
        "\n",
        "#0값은 보라, 1의 값은 노랑, 테스트 데이터는 빨간색"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7-cNCBuVDKU",
        "outputId": "ce13574d-9177-42a7-e7b5-12d61132d75b"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) # 각각의 슬라이스들이 1행2열과 1행1열의 6개 슬라이스로 만들어져서 dataset에 들어가짐.\n",
        "\n",
        "W = tf.Variable(tf.zeros([2,1]), name = 'weight') #여기서 난 w를 2행 1열로 만들어줄 건데, 그 이유는, X가 2열이고, 결과값 Y가 1열이기 때문임. zeros한 이유는 그냥.. 어짜피 w 초기값은 아무거나 넣으니까.\n",
        "b = tf.Variable(tf.zeros([1]), name = 'bias') #바이어스 값을 더할 거니까 그냥 이렇게.\n",
        "\n",
        "def logistic_regression(features):  #가설함수인 logistic regression함수 만들 거임.\n",
        "  hypothesis = tf.divide(1., 1. + tf.exp(-tf.matmul(features, W)+b)) #시그모이드 함수 만들거임. matmul은 리니어에서 H(x) = WX 이거니까. 이거 행렬곱으로 했던 거 쓰는 거! z = H(x) = WX\n",
        "  return hypothesis\n",
        "\n",
        "def loss_fn(hypothesis, labels): #위에서 가설함수 정했으면, 비용함수도 정해주면 됨. 비용함수는 loss_fn이라는 이름으로! 뭐 손실함수와 같은듯. hypotesis는 가설(예측값), label은 y(실제값)\n",
        "  cost = -tf.reduce_mean( labels * tf.math.log(hypothesis) + (1-labels) * tf.math.log(1 - hypothesis) )\n",
        "  return cost\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01) #자동 최적화도구. 이거만 쓰면 굳이 경사하강법 알고리즘 구현 안해도 됨.\n",
        "\n",
        "def accuracy_fn(hypothesis, labels): #여기서 정확도 함수 만들어줄건데, 0.5를 기준으로 넘으면 1, 안넘으면 0으로 캐스팅 해줄 거다.\n",
        "  predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "  accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels),dtype=tf.int32)) #라벨값, y, 실제값은 0과 1로 되어있기 때문에, predicted에서 0과 1로 캐스팅 된 값과 비교. 비교해서 같으면 값을 올려준다. 근데 이걸 평균함수 쓰니까 알아서 올리고 나눠서 평균(정확도) 구해줌.\n",
        "  return accuracy                                                              #한번 더 캐스팅하는 이유와, 정수형으로 바꿔주는 이유가 좀 궁금한데?\n",
        "\n",
        "def grad(features, labels): #grad를 미분값을 반환하는 함수로 만들거임.\n",
        "  with tf.GradientTape() as tape:\n",
        "    hypothesis = logistic_regression(features)    #허허 여기 뭐지 대체..?\n",
        "    loss_value = loss_fn(hypothesis, labels)\n",
        "  return tape.gradient(loss_value, [W,b]) #실제 가설 함수를 통해 나온 값과, 손실함수를 통해 나온 비용의 미분값을 계산해서 반환하는 함수.\n",
        "\n",
        "#학습시작\n",
        "#여기서부터 아래까지는 뭐 그냥 설명을 안하고 넘어가네.\n",
        "EPOCHS = 1001\n",
        "\n",
        "for step in range(EPOCHS):\n",
        "    for features, labels  in iter(dataset.batch(len(x_train))): #batch는 원래 몇개 불러올지 정하는 건데, 데이터가 6개뿐이라 그냥 다 불러왔음. iter는 반복을 끝낼 값을 지정하면 특정 값이 나올 때 반복을 끝냄.\n",
        "        hypothesis = logistic_regression(features)\n",
        "        grads = grad(features, labels)\n",
        "        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "        if step % 100 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, loss_fn(hypothesis,labels)))\n",
        "\n",
        "  # Iter: 0, Loss: 0.6931\n",
        "  # Iter: 100, Loss: 0.5781\n",
        "  # Iter: 200, Loss: 0.5352\n",
        "  # Iter: 300, Loss: 0.5056\n",
        "  # Iter: 400, Loss: 0.4840\n",
        "  # Iter: 500, Loss: 0.4673\n",
        "  # Iter: 600, Loss: 0.4537\n",
        "  # Iter: 700, Loss: 0.4421\n",
        "  # Iter: 800, Loss: 0.4320\n",
        "  # Iter: 900, Loss: 0.4229\n",
        "  # Iter: 1000, Loss: 0.4145\n",
        "#새로운 Test Data를 통한 검증 수행을 [5,2]의 Data로 테스트 수행합니다.  (그래프상 1이 나와야 정상입니다)\n",
        "\n",
        "#결과값 여기도 뭐 설명을 해줘야 알지\n",
        "test_acc = accuracy_fn(logistic_regression(x_test),y_test)\n",
        "print(\"Test Result = {}\".format(tf.cast(logistic_regression(x_test) > 0.5, dtype=tf.int32)))  \n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))\n",
        "\n",
        "# Test Result = [[1]]\n",
        "# Testset Accuracy: 1.0000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter: 0, Loss: 0.6931\n",
            "Iter: 100, Loss: 0.5781\n",
            "Iter: 200, Loss: 0.5352\n",
            "Iter: 300, Loss: 0.5056\n",
            "Iter: 400, Loss: 0.4840\n",
            "Iter: 500, Loss: 0.4673\n",
            "Iter: 600, Loss: 0.4537\n",
            "Iter: 700, Loss: 0.4421\n",
            "Iter: 800, Loss: 0.4320\n",
            "Iter: 900, Loss: 0.4229\n",
            "Iter: 1000, Loss: 0.4145\n",
            "Test Result = [[1]]\n",
            "Testset Accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjLSg-wVkXW2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7jDG8aRbVWx"
      },
      "source": [
        "##Lab 06 Softmax Zoo_classifier-eager\n",
        "Softmax를 사용하여 Zoo 데이터를 활용하여 분류를 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XG7q1mZSbg_1",
        "outputId": "a342c9d2-0e43-4935-92bd-709ccfb9243b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "tf.random.set_seed(777)  # for reproducibility"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGv6SrwNbhMN",
        "outputId": "1da8022b-c7a9-484d-de43-ddc050035b05"
      },
      "source": [
        "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, -1]\n",
        "\n",
        "nb_classes = 7  # 0 ~ 6 #데이터가 7까지 있음.\n",
        "\n",
        "# Make Y data as onehot shape\n",
        "Y_one_hot = tf.one_hot(y_data.astype(np.int32), nb_classes)   #y값을 원핫벡터로 바꿔 보면, 열이 몇개 나오는지 바로 확인 가능. 그래서 7열로 출력됨.\n",
        "\n",
        "print(x_data.shape, Y_one_hot.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(101, 16) (101, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho3e6XtIbriv"
      },
      "source": [
        "\n",
        "#Weight and bias setting\n",
        "W = tf.Variable(tf.random.normal((16, nb_classes)), name='weight') #shape로 확인해보니, 인풋데이터 x가 16열, 아웃풋 데이터 y가 7열임. nb_class를 그거에 맞게 7열로 했음. #원핫벡터용.\n",
        "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')   #bias는 원래 아웃풋데이터 열하고 동일하게 들어감.\n",
        "variables = [W, b]    #이거 되게 좋았던 게 나중에 grad함수 만들 때 거기서 tape(함수, 변수) 들어가는데 그거 생각해서 담아놓은 거.\n",
        "\n",
        "# tf.nn.softmax computes softmax activations\n",
        "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
        "\n",
        "def logit_fn(X):  #이거 하는 이유는 g(z)해야하잖아? 그래서 z = H(x) = WX 이거 미리 해놓은 거임. z 만들기용.\n",
        "    return tf.matmul(X, W) + b\n",
        "\n",
        "def hypothesis(X):  #이 가설함수에서 이제 g()에 z를 넣은 거지. 아까 만든 z 넣은 거고.\n",
        "    return tf.nn.softmax(logit_fn(X)) #소프트맥스 함수 g(z) 이렇게 해서 총합이 1인 0~1사이 값 해놓음.\n",
        "\n",
        "def cost_fn(X, Y):\n",
        "    logits = logit_fn(X)\n",
        "    cost_i = tf.keras.losses.categorical_crossentropy(y_true=Y, y_pred=logits, #이 안에 들어가는 값들이 이해가 안되긴 하는데, 대충 흐름은 지금 다 파악했다.\n",
        "                                                      from_logits=True)    #크로스 엔트로피 해주는 함수임. 크로스 엔트로피 해주면 자동으로 비용측정 가능. logistic할 때 했던 함수 구현 안해도 됨.\n",
        "    cost = tf.reduce_mean(cost_i)    \n",
        "    return cost\n",
        "\n",
        "def grad_fn(X, Y):    # 가설함수 세우고, 비용함수 세웠잖아? 그 다음엔 뭐다? 비용최소화. 얘도 경사하강법 사용함. 경사하강법 쓴다는 건, 미분해가면서 하는 그거 알지? W := W - 학습률*미분한 비용함수\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = cost_fn(X, Y)\n",
        "        grads = tape.gradient(loss, variables)\n",
        "        return grads\n",
        "    \n",
        "def prediction(X, Y):     #가설과 정답이 얼마나 유사한지 확인하는 예측함수. \n",
        "    pred = tf.argmax(hypothesis(X), 1) #argmax이거로 원핫 인코딩. 예측값 원핫 인코딩.\n",
        "    correct_prediction = tf.equal(pred, tf.argmax(Y, 1))  #여기선 실제 값 원핫 인코딩.  pred에서 원핫 인코딩한 예측값 x가 실제값 y와 같은지 확인해보는 거.  \n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  #확인해본 뒤, 위에 코드 equa에서 TRUE FALSE로 나오는 것 0과 1로 캐스팅해서 평균값 구하기. 이게 정확도.\n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qj4YVboUbsdg",
        "outputId": "865ff0e7-0ad1-4463-b11a-43050259807f"
      },
      "source": [
        "def fit(X, Y, epochs=1000, verbose=100):  #fit 함수인데. optimizer로 자동최적화 시키는 거.\n",
        "    optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "    for i in range(epochs): #여긴 뭐 여전히 이해안되고.. 질문하자!\n",
        "        grads = grad_fn(X, Y)\n",
        "        optimizer.apply_gradients(zip(grads, variables)) #여기도 이해 안되고.\n",
        "        if (i==0) | ((i+1)%verbose==0): #여긴 아예 모르겠는데 ㅋㅋㅋㅋ\n",
        "#             print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
        "            acc = prediction(X, Y).numpy()\n",
        "            loss = cost_fn(X, Y).numpy() \n",
        "            print('Steps: {} Loss: {}, Acc: {}'.format(i+1, loss, acc))  #사실 난 여기 포매팅도 잘 이해 안됨.\n",
        "\n",
        "fit(x_data, Y_one_hot)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 1 Loss: 3.635028839111328, Acc: 0.1683168262243271\n",
            "Steps: 100 Loss: 0.5194157958030701, Acc: 0.7920792102813721\n",
            "Steps: 200 Loss: 0.31850090622901917, Acc: 0.9108911156654358\n",
            "Steps: 300 Loss: 0.23534876108169556, Acc: 0.9405940771102905\n",
            "Steps: 400 Loss: 0.18872135877609253, Acc: 0.9504950642585754\n",
            "Steps: 500 Loss: 0.158460333943367, Acc: 0.9504950642585754\n",
            "Steps: 600 Loss: 0.13703754544258118, Acc: 0.9900990128517151\n",
            "Steps: 700 Loss: 0.12098980695009232, Acc: 0.9900990128517151\n",
            "Steps: 800 Loss: 0.10847961902618408, Acc: 1.0\n",
            "Steps: 900 Loss: 0.09843038767576218, Acc: 1.0\n",
            "Steps: 1000 Loss: 0.09016557037830353, Acc: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7PdUeERbu9t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}